{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying Urban sounds using Deep Learning\n",
    "\n",
    "## 4 Model Refinement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Preprocessed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the preprocessed data from previous notebook\n",
    "\n",
    "%store -r x_train \n",
    "%store -r x_test \n",
    "%store -r y_train \n",
    "%store -r y_test \n",
    "%store -r yy \n",
    "%store -r le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model refinement\n",
    "\n",
    "In our inital attempt, we were able to achieve a Classification Accuracy score of: \n",
    "\n",
    "* Training data Accuracy:  92.3% \n",
    "* Testing data Accuracy:  87% \n",
    "\n",
    "We will now see if we can improve upon that score using a Convolutional Neural Network (CNN). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extraction refinement \n",
    "\n",
    "In the prevous feature extraction stage, the MFCC vectors would vary in size for the different audio files (depending on the samples duration). \n",
    "\n",
    "However, CNNs require a fixed size for all inputs. To overcome this we will zero pad the output vectors to make them all the same size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "max_pad_len = 174\n",
    "\n",
    "def extract_features(file_name):\n",
    "   \n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "        pad_width = max_pad_len - mfccs.shape[1]\n",
    "        mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error encountered while parsing file: \", file_name)\n",
    "        return None \n",
    "     \n",
    "    return mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\envs\\project\\lib\\site-packages\\librosa\\core\\spectrum.py:222: UserWarning: n_fft=2048 is too small for input signal of length=1323\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\envs\\project\\lib\\site-packages\\librosa\\core\\spectrum.py:222: UserWarning: n_fft=2048 is too small for input signal of length=1103\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\anaconda3\\envs\\project\\lib\\site-packages\\librosa\\core\\spectrum.py:222: UserWarning: n_fft=2048 is too small for input signal of length=1523\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished feature extraction from  8732  files\n"
     ]
    }
   ],
   "source": [
    "# Load various imports \n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "\n",
    "# Set the path to the full UrbanSound dataset \n",
    "fulldatasetpath = 'C:/Users/user/Udacity-ML-Capstone/UrbanSound8K/audio/'\n",
    "\n",
    "metadata = pd.read_csv('../UrbanSound Dataset sample/metadata/UrbanSound8K.csv')\n",
    "\n",
    "features = []\n",
    "\n",
    "# Iterate through each sound file and extract the features \n",
    "for index, row in metadata.iterrows():\n",
    "    \n",
    "    file_name = os.path.join(os.path.abspath(fulldatasetpath),'fold'+str(row[\"fold\"])+'/',str(row[\"slice_file_name\"]))\n",
    "    \n",
    "    class_label = row[\"class_name\"]\n",
    "    data = extract_features(file_name)\n",
    "    \n",
    "    features.append([data, class_label])\n",
    "\n",
    "# Convert into a Panda dataframe \n",
    "featuresdf = pd.DataFrame(features, columns=['feature','class_label'])\n",
    "\n",
    "print('Finished feature extraction from ', len(featuresdf), ' files') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Convert features and corresponding classification labels into numpy arrays\n",
    "X = np.array(featuresdf.feature.tolist())\n",
    "y = np.array(featuresdf.class_label.tolist())\n",
    "\n",
    "# Encode the classification labels\n",
    "le = LabelEncoder()\n",
    "yy = to_categorical(le.fit_transform(y)) \n",
    "\n",
    "# split the dataset \n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, yy, test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network (CNN) model architecture \n",
    "\n",
    "\n",
    "We will modify our model to be a Convolutional Neural Network (CNN) again using Keras and a Tensorflow backend. \n",
    "\n",
    "Again we will use a `sequential` model, starting with a simple model architecture, consisting of four `Conv2D` convolution layers, with our final output layer being a `dense` layer. \n",
    "\n",
    "The convolution layers are designed for feature detection. It works by sliding a filter window over the input and performing a matrix multiplication and storing the result in a feature map. This operation is known as a convolution. \n",
    "\n",
    "\n",
    "The `filter` parameter specifies the number of nodes in each layer. Each layer will increase in size from 16, 32, 64 to 128, while the `kernel_size` parameter specifies the size of the kernel window which in this case is 2 resulting in a 2x2 filter matrix. \n",
    "\n",
    "The first layer will receive the input shape of (40, 174, 1) where 40 is the number of MFCC's 174 is the number of frames taking padding into account and the 1 signifying that the audio is mono. \n",
    "\n",
    "The activation function we will be using for our convolutional layers is `ReLU` which is the same as our previous model. We will use a smaller `Dropout` value of 20% on our convolutional layers. \n",
    "\n",
    "Each convolutional layer has an associated pooling layer of `MaxPooling2D` type with the final convolutional layer having a `GlobalAveragePooling2D` type. The pooling layer is do reduce the dimensionality of the model (by reducing the parameters and subsquent computation requirements) which serves to shorten the training time and reduce overfitting. The Max Pooling type takes the maximum size for each window and the Global Average Pooling type takes the average which is suitable for feeding into our `dense` output layer.  \n",
    "\n",
    "Our output layer will have 10 nodes (num_labels) which matches the number of possible classifications. The activation is for our output layer is `softmax`. Softmax makes the output sum up to 1 so the output can be interpreted as probabilities. The model will then make its prediction based on which option has the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from sklearn import metrics \n",
    "\n",
    "num_rows = 40\n",
    "num_columns = 174\n",
    "num_channels = 1\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], num_rows, num_columns, num_channels)\n",
    "x_test = x_test.reshape(x_test.shape[0], num_rows, num_columns, num_channels)\n",
    "\n",
    "num_labels = yy.shape[1]\n",
    "filter_size = 2\n",
    "\n",
    "# Construct model \n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, input_shape=(num_rows, num_columns, num_channels), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=2, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "model.add(Dense(num_labels, activation='softmax')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model \n",
    "\n",
    "For compiling our model, we will use the same three parameters as the previous model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_11 (Conv2D)           (None, 39, 173, 16)       80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 19, 86, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 18, 85, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 9, 42, 32)         0         \n",
      "_________________________________________________________________\n",
      "dropout_18 (Dropout)         (None, 9, 42, 32)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 8, 41, 64)         8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_13 (MaxPooling (None, 4, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 4, 20, 64)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 3, 19, 128)        32896     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 1, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 1, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 44,602\n",
      "Trainable params: 44,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "1747/1747 [==============================] - 9s 5ms/step\n",
      "Pre-training accuracy: 12.0206%\n"
     ]
    }
   ],
   "source": [
    "# Display model architecture summary \n",
    "model.summary()\n",
    "\n",
    "# Calculate pre-training accuracy \n",
    "score = model.evaluate(x_test, y_test, verbose=1)\n",
    "accuracy = 100*score[1]\n",
    "\n",
    "print(\"Pre-training accuracy: %.4f%%\" % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "\n",
    "Here we will train the model. As training a CNN can take a sigificant amount of time, we will start with a low number of epochs and a low batch size. If we can see from the output that the model is converging, we will increase both numbers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 6.8757 - accuracy: 0.1549\n",
      "Epoch 00001: val_loss improved from inf to 2.16166, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 24s 855ms/step - loss: 6.8757 - accuracy: 0.1549 - val_loss: 2.1617 - val_accuracy: 0.2272\n",
      "Epoch 2/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 2.2931 - accuracy: 0.2481\n",
      "Epoch 00002: val_loss improved from 2.16166 to 1.97656, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 762ms/step - loss: 2.2931 - accuracy: 0.2481 - val_loss: 1.9766 - val_accuracy: 0.3017\n",
      "Epoch 3/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.8738 - accuracy: 0.3374\n",
      "Epoch 00003: val_loss improved from 1.97656 to 1.80319, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 23s 829ms/step - loss: 1.8738 - accuracy: 0.3374 - val_loss: 1.8032 - val_accuracy: 0.3927\n",
      "Epoch 4/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.6604 - accuracy: 0.4229\n",
      "Epoch 00004: val_loss improved from 1.80319 to 1.65072, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 23s 817ms/step - loss: 1.6604 - accuracy: 0.4229 - val_loss: 1.6507 - val_accuracy: 0.4528\n",
      "Epoch 5/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.5095 - accuracy: 0.4677\n",
      "Epoch 00005: val_loss improved from 1.65072 to 1.50073, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 756ms/step - loss: 1.5095 - accuracy: 0.4677 - val_loss: 1.5007 - val_accuracy: 0.5152\n",
      "Epoch 6/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.4064 - accuracy: 0.5077\n",
      "Epoch 00006: val_loss improved from 1.50073 to 1.43515, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 765ms/step - loss: 1.4064 - accuracy: 0.5077 - val_loss: 1.4352 - val_accuracy: 0.5215\n",
      "Epoch 7/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.3119 - accuracy: 0.5396\n",
      "Epoch 00007: val_loss improved from 1.43515 to 1.36146, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 761ms/step - loss: 1.3119 - accuracy: 0.5396 - val_loss: 1.3615 - val_accuracy: 0.5369\n",
      "Epoch 8/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.2666 - accuracy: 0.5568\n",
      "Epoch 00008: val_loss improved from 1.36146 to 1.30493, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 765ms/step - loss: 1.2666 - accuracy: 0.5568 - val_loss: 1.3049 - val_accuracy: 0.5558\n",
      "Epoch 9/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.2284 - accuracy: 0.5674\n",
      "Epoch 00009: val_loss improved from 1.30493 to 1.28175, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 764ms/step - loss: 1.2284 - accuracy: 0.5674 - val_loss: 1.2817 - val_accuracy: 0.5718\n",
      "Epoch 10/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.1959 - accuracy: 0.5802\n",
      "Epoch 00010: val_loss improved from 1.28175 to 1.25276, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 762ms/step - loss: 1.1959 - accuracy: 0.5802 - val_loss: 1.2528 - val_accuracy: 0.5741\n",
      "Epoch 11/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.1417 - accuracy: 0.5951\n",
      "Epoch 00011: val_loss improved from 1.25276 to 1.20612, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 762ms/step - loss: 1.1417 - accuracy: 0.5951 - val_loss: 1.2061 - val_accuracy: 0.6165\n",
      "Epoch 12/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.1023 - accuracy: 0.6168\n",
      "Epoch 00012: val_loss improved from 1.20612 to 1.16643, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 762ms/step - loss: 1.1023 - accuracy: 0.6168 - val_loss: 1.1664 - val_accuracy: 0.6056\n",
      "Epoch 13/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.0610 - accuracy: 0.6288\n",
      "Epoch 00013: val_loss improved from 1.16643 to 1.12802, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 764ms/step - loss: 1.0610 - accuracy: 0.6288 - val_loss: 1.1280 - val_accuracy: 0.6171\n",
      "Epoch 14/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.0491 - accuracy: 0.6359\n",
      "Epoch 00014: val_loss improved from 1.12802 to 1.10999, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 22s 771ms/step - loss: 1.0491 - accuracy: 0.6359 - val_loss: 1.1100 - val_accuracy: 0.6291\n",
      "Epoch 15/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 1.0017 - accuracy: 0.6584\n",
      "Epoch 00015: val_loss improved from 1.10999 to 1.08341, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 757ms/step - loss: 1.0017 - accuracy: 0.6584 - val_loss: 1.0834 - val_accuracy: 0.6417\n",
      "Epoch 16/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.9898 - accuracy: 0.6577\n",
      "Epoch 00016: val_loss improved from 1.08341 to 1.07541, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 767ms/step - loss: 0.9898 - accuracy: 0.6577 - val_loss: 1.0754 - val_accuracy: 0.6537\n",
      "Epoch 17/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.9582 - accuracy: 0.6649\n",
      "Epoch 00017: val_loss improved from 1.07541 to 1.02965, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 760ms/step - loss: 0.9582 - accuracy: 0.6649 - val_loss: 1.0296 - val_accuracy: 0.6720\n",
      "Epoch 18/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.9130 - accuracy: 0.6840\n",
      "Epoch 00018: val_loss improved from 1.02965 to 0.99649, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 764ms/step - loss: 0.9130 - accuracy: 0.6840 - val_loss: 0.9965 - val_accuracy: 0.6880\n",
      "Epoch 19/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.8908 - accuracy: 0.6922\n",
      "Epoch 00019: val_loss improved from 0.99649 to 0.97384, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 764ms/step - loss: 0.8908 - accuracy: 0.6922 - val_loss: 0.9738 - val_accuracy: 0.6823\n",
      "Epoch 20/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.8739 - accuracy: 0.7065\n",
      "Epoch 00020: val_loss improved from 0.97384 to 0.94193, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 763ms/step - loss: 0.8739 - accuracy: 0.7065 - val_loss: 0.9419 - val_accuracy: 0.7092\n",
      "Epoch 21/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.8340 - accuracy: 0.7135\n",
      "Epoch 00021: val_loss improved from 0.94193 to 0.92346, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 764ms/step - loss: 0.8340 - accuracy: 0.7135 - val_loss: 0.9235 - val_accuracy: 0.6915\n",
      "Epoch 22/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.8325 - accuracy: 0.7117\n",
      "Epoch 00022: val_loss improved from 0.92346 to 0.88564, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 764ms/step - loss: 0.8325 - accuracy: 0.7117 - val_loss: 0.8856 - val_accuracy: 0.7172\n",
      "Epoch 23/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.8220 - accuracy: 0.7152\n",
      "Epoch 00023: val_loss did not improve from 0.88564\n",
      "28/28 [==============================] - 21s 763ms/step - loss: 0.8220 - accuracy: 0.7152 - val_loss: 0.8873 - val_accuracy: 0.7109\n",
      "Epoch 24/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7798 - accuracy: 0.7333\n",
      "Epoch 00024: val_loss improved from 0.88564 to 0.88399, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 765ms/step - loss: 0.7798 - accuracy: 0.7333 - val_loss: 0.8840 - val_accuracy: 0.7104\n",
      "Epoch 25/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7629 - accuracy: 0.7426\n",
      "Epoch 00025: val_loss improved from 0.88399 to 0.84165, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 760ms/step - loss: 0.7629 - accuracy: 0.7426 - val_loss: 0.8417 - val_accuracy: 0.7338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7562 - accuracy: 0.7396\n",
      "Epoch 00026: val_loss improved from 0.84165 to 0.83587, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 758ms/step - loss: 0.7562 - accuracy: 0.7396 - val_loss: 0.8359 - val_accuracy: 0.7338\n",
      "Epoch 27/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7396 - accuracy: 0.7520\n",
      "Epoch 00027: val_loss improved from 0.83587 to 0.80694, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 757ms/step - loss: 0.7396 - accuracy: 0.7520 - val_loss: 0.8069 - val_accuracy: 0.7413\n",
      "Epoch 28/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7385 - accuracy: 0.7500\n",
      "Epoch 00028: val_loss improved from 0.80694 to 0.79338, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 757ms/step - loss: 0.7385 - accuracy: 0.7500 - val_loss: 0.7934 - val_accuracy: 0.7510\n",
      "Epoch 29/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.7057 - accuracy: 0.7603\n",
      "Epoch 00029: val_loss improved from 0.79338 to 0.76853, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 761ms/step - loss: 0.7057 - accuracy: 0.7603 - val_loss: 0.7685 - val_accuracy: 0.7573\n",
      "Epoch 30/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6830 - accuracy: 0.7705\n",
      "Epoch 00030: val_loss improved from 0.76853 to 0.74553, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 759ms/step - loss: 0.6830 - accuracy: 0.7705 - val_loss: 0.7455 - val_accuracy: 0.7831\n",
      "Epoch 31/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6778 - accuracy: 0.7725\n",
      "Epoch 00031: val_loss improved from 0.74553 to 0.72673, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 20s 726ms/step - loss: 0.6778 - accuracy: 0.7725 - val_loss: 0.7267 - val_accuracy: 0.7733\n",
      "Epoch 32/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6584 - accuracy: 0.7744\n",
      "Epoch 00032: val_loss improved from 0.72673 to 0.68795, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 20s 727ms/step - loss: 0.6584 - accuracy: 0.7744 - val_loss: 0.6880 - val_accuracy: 0.7928\n",
      "Epoch 33/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6335 - accuracy: 0.7835\n",
      "Epoch 00033: val_loss did not improve from 0.68795\n",
      "28/28 [==============================] - 20s 725ms/step - loss: 0.6335 - accuracy: 0.7835 - val_loss: 0.7158 - val_accuracy: 0.7802\n",
      "Epoch 34/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6297 - accuracy: 0.7880\n",
      "Epoch 00034: val_loss improved from 0.68795 to 0.67252, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 20s 725ms/step - loss: 0.6297 - accuracy: 0.7880 - val_loss: 0.6725 - val_accuracy: 0.7894\n",
      "Epoch 35/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6194 - accuracy: 0.7938\n",
      "Epoch 00035: val_loss did not improve from 0.67252\n",
      "28/28 [==============================] - 20s 726ms/step - loss: 0.6194 - accuracy: 0.7938 - val_loss: 0.7033 - val_accuracy: 0.7762\n",
      "Epoch 36/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.6117 - accuracy: 0.7874\n",
      "Epoch 00036: val_loss improved from 0.67252 to 0.66119, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 20s 726ms/step - loss: 0.6117 - accuracy: 0.7874 - val_loss: 0.6612 - val_accuracy: 0.7985\n",
      "Epoch 37/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5952 - accuracy: 0.7989\n",
      "Epoch 00037: val_loss improved from 0.66119 to 0.65744, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 20s 728ms/step - loss: 0.5952 - accuracy: 0.7989 - val_loss: 0.6574 - val_accuracy: 0.7871\n",
      "Epoch 38/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5776 - accuracy: 0.8073\n",
      "Epoch 00038: val_loss improved from 0.65744 to 0.61918, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 737ms/step - loss: 0.5776 - accuracy: 0.8073 - val_loss: 0.6192 - val_accuracy: 0.8134\n",
      "Epoch 39/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5812 - accuracy: 0.8042\n",
      "Epoch 00039: val_loss did not improve from 0.61918\n",
      "28/28 [==============================] - 23s 806ms/step - loss: 0.5812 - accuracy: 0.8042 - val_loss: 0.6498 - val_accuracy: 0.8002\n",
      "Epoch 40/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5599 - accuracy: 0.8079\n",
      "Epoch 00040: val_loss did not improve from 0.61918\n",
      "28/28 [==============================] - 21s 759ms/step - loss: 0.5599 - accuracy: 0.8079 - val_loss: 0.6501 - val_accuracy: 0.7922\n",
      "Epoch 41/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5489 - accuracy: 0.8157\n",
      "Epoch 00041: val_loss did not improve from 0.61918\n",
      "28/28 [==============================] - 21s 755ms/step - loss: 0.5489 - accuracy: 0.8157 - val_loss: 0.6319 - val_accuracy: 0.8019\n",
      "Epoch 42/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5422 - accuracy: 0.8180\n",
      "Epoch 00042: val_loss improved from 0.61918 to 0.61852, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 755ms/step - loss: 0.5422 - accuracy: 0.8180 - val_loss: 0.6185 - val_accuracy: 0.8088\n",
      "Epoch 43/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5432 - accuracy: 0.8179\n",
      "Epoch 00043: val_loss improved from 0.61852 to 0.58879, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 758ms/step - loss: 0.5432 - accuracy: 0.8179 - val_loss: 0.5888 - val_accuracy: 0.8237\n",
      "Epoch 44/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5276 - accuracy: 0.8202\n",
      "Epoch 00044: val_loss improved from 0.58879 to 0.57757, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 760ms/step - loss: 0.5276 - accuracy: 0.8202 - val_loss: 0.5776 - val_accuracy: 0.8174\n",
      "Epoch 45/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.5013 - accuracy: 0.8361\n",
      "Epoch 00045: val_loss improved from 0.57757 to 0.56677, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 759ms/step - loss: 0.5013 - accuracy: 0.8361 - val_loss: 0.5668 - val_accuracy: 0.8243\n",
      "Epoch 46/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4955 - accuracy: 0.8329\n",
      "Epoch 00046: val_loss improved from 0.56677 to 0.54043, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 755ms/step - loss: 0.4955 - accuracy: 0.8329 - val_loss: 0.5404 - val_accuracy: 0.8363\n",
      "Epoch 47/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4847 - accuracy: 0.8364\n",
      "Epoch 00047: val_loss improved from 0.54043 to 0.53500, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 22s 787ms/step - loss: 0.4847 - accuracy: 0.8364 - val_loss: 0.5350 - val_accuracy: 0.8380\n",
      "Epoch 48/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4768 - accuracy: 0.8425\n",
      "Epoch 00048: val_loss did not improve from 0.53500\n",
      "28/28 [==============================] - 21s 754ms/step - loss: 0.4768 - accuracy: 0.8425 - val_loss: 0.5577 - val_accuracy: 0.8231\n",
      "Epoch 49/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4602 - accuracy: 0.8388\n",
      "Epoch 00049: val_loss improved from 0.53500 to 0.51960, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 759ms/step - loss: 0.4602 - accuracy: 0.8388 - val_loss: 0.5196 - val_accuracy: 0.8426\n",
      "Epoch 50/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4623 - accuracy: 0.8432\n",
      "Epoch 00050: val_loss did not improve from 0.51960\n",
      "28/28 [==============================] - 21s 760ms/step - loss: 0.4623 - accuracy: 0.8432 - val_loss: 0.5203 - val_accuracy: 0.8420\n",
      "Epoch 51/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4558 - accuracy: 0.8471\n",
      "Epoch 00051: val_loss did not improve from 0.51960\n",
      "28/28 [==============================] - 21s 758ms/step - loss: 0.4558 - accuracy: 0.8471 - val_loss: 0.5348 - val_accuracy: 0.8346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4517 - accuracy: 0.8513\n",
      "Epoch 00052: val_loss improved from 0.51960 to 0.49570, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 754ms/step - loss: 0.4517 - accuracy: 0.8513 - val_loss: 0.4957 - val_accuracy: 0.8392\n",
      "Epoch 53/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4320 - accuracy: 0.8518\n",
      "Epoch 00053: val_loss improved from 0.49570 to 0.46987, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 755ms/step - loss: 0.4320 - accuracy: 0.8518 - val_loss: 0.4699 - val_accuracy: 0.8552\n",
      "Epoch 54/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4199 - accuracy: 0.8586\n",
      "Epoch 00054: val_loss did not improve from 0.46987\n",
      "28/28 [==============================] - 21s 754ms/step - loss: 0.4199 - accuracy: 0.8586 - val_loss: 0.4839 - val_accuracy: 0.8512\n",
      "Epoch 55/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4248 - accuracy: 0.8583\n",
      "Epoch 00055: val_loss did not improve from 0.46987\n",
      "28/28 [==============================] - 21s 754ms/step - loss: 0.4248 - accuracy: 0.8583 - val_loss: 0.5052 - val_accuracy: 0.8432\n",
      "Epoch 56/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4337 - accuracy: 0.8508\n",
      "Epoch 00056: val_loss did not improve from 0.46987\n",
      "28/28 [==============================] - 22s 780ms/step - loss: 0.4337 - accuracy: 0.8508 - val_loss: 0.4965 - val_accuracy: 0.8392\n",
      "Epoch 57/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.4296 - accuracy: 0.8584\n",
      "Epoch 00057: val_loss improved from 0.46987 to 0.46461, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 22s 783ms/step - loss: 0.4296 - accuracy: 0.8584 - val_loss: 0.4646 - val_accuracy: 0.8517\n",
      "Epoch 58/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3959 - accuracy: 0.8681\n",
      "Epoch 00058: val_loss did not improve from 0.46461\n",
      "28/28 [==============================] - 21s 755ms/step - loss: 0.3959 - accuracy: 0.8681 - val_loss: 0.4708 - val_accuracy: 0.8563\n",
      "Epoch 59/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3939 - accuracy: 0.8621\n",
      "Epoch 00059: val_loss improved from 0.46461 to 0.44319, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 755ms/step - loss: 0.3939 - accuracy: 0.8621 - val_loss: 0.4432 - val_accuracy: 0.8603\n",
      "Epoch 60/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3863 - accuracy: 0.8690\n",
      "Epoch 00060: val_loss did not improve from 0.44319\n",
      "28/28 [==============================] - 21s 764ms/step - loss: 0.3863 - accuracy: 0.8690 - val_loss: 0.4803 - val_accuracy: 0.8552\n",
      "Epoch 61/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3926 - accuracy: 0.8684\n",
      "Epoch 00061: val_loss improved from 0.44319 to 0.44155, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 756ms/step - loss: 0.3926 - accuracy: 0.8684 - val_loss: 0.4416 - val_accuracy: 0.8643\n",
      "Epoch 62/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3590 - accuracy: 0.8764\n",
      "Epoch 00062: val_loss improved from 0.44155 to 0.42528, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 760ms/step - loss: 0.3590 - accuracy: 0.8764 - val_loss: 0.4253 - val_accuracy: 0.8638\n",
      "Epoch 63/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3709 - accuracy: 0.8727\n",
      "Epoch 00063: val_loss did not improve from 0.42528\n",
      "28/28 [==============================] - 21s 757ms/step - loss: 0.3709 - accuracy: 0.8727 - val_loss: 0.4423 - val_accuracy: 0.8615\n",
      "Epoch 64/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3420 - accuracy: 0.8839\n",
      "Epoch 00064: val_loss did not improve from 0.42528\n",
      "28/28 [==============================] - 21s 754ms/step - loss: 0.3420 - accuracy: 0.8839 - val_loss: 0.4263 - val_accuracy: 0.8672\n",
      "Epoch 65/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3481 - accuracy: 0.8815\n",
      "Epoch 00065: val_loss improved from 0.42528 to 0.41194, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 758ms/step - loss: 0.3481 - accuracy: 0.8815 - val_loss: 0.4119 - val_accuracy: 0.8729\n",
      "Epoch 66/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3495 - accuracy: 0.8806\n",
      "Epoch 00066: val_loss improved from 0.41194 to 0.40596, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 760ms/step - loss: 0.3495 - accuracy: 0.8806 - val_loss: 0.4060 - val_accuracy: 0.8735\n",
      "Epoch 67/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3517 - accuracy: 0.8792\n",
      "Epoch 00067: val_loss did not improve from 0.40596\n",
      "28/28 [==============================] - 21s 756ms/step - loss: 0.3517 - accuracy: 0.8792 - val_loss: 0.4235 - val_accuracy: 0.8769\n",
      "Epoch 68/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3518 - accuracy: 0.8789\n",
      "Epoch 00068: val_loss improved from 0.40596 to 0.39707, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 759ms/step - loss: 0.3518 - accuracy: 0.8789 - val_loss: 0.3971 - val_accuracy: 0.8844\n",
      "Epoch 69/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3323 - accuracy: 0.8859\n",
      "Epoch 00069: val_loss improved from 0.39707 to 0.38100, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 22s 777ms/step - loss: 0.3323 - accuracy: 0.8859 - val_loss: 0.3810 - val_accuracy: 0.8809\n",
      "Epoch 70/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3177 - accuracy: 0.8893\n",
      "Epoch 00070: val_loss did not improve from 0.38100\n",
      "28/28 [==============================] - 21s 758ms/step - loss: 0.3177 - accuracy: 0.8893 - val_loss: 0.3915 - val_accuracy: 0.8895\n",
      "Epoch 71/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3258 - accuracy: 0.8906\n",
      "Epoch 00071: val_loss improved from 0.38100 to 0.37806, saving model to saved_models\\weights.best.basic_cnn.hdf5\n",
      "28/28 [==============================] - 21s 764ms/step - loss: 0.3258 - accuracy: 0.8906 - val_loss: 0.3781 - val_accuracy: 0.8855\n",
      "Epoch 72/72\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.3160 - accuracy: 0.8926\n",
      "Epoch 00072: val_loss did not improve from 0.37806\n",
      "28/28 [==============================] - 21s 753ms/step - loss: 0.3160 - accuracy: 0.8926 - val_loss: 0.4146 - val_accuracy: 0.8683\n",
      "Training completed in time:  0:26:30.424859\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint \n",
    "from datetime import datetime \n",
    "\n",
    "#num_epochs = 12\n",
    "#num_batch_size = 128\n",
    "\n",
    "num_epochs = 72\n",
    "num_batch_size = 256\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.basic_cnn.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "start = datetime.now()\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(x_test, y_test), callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the model \n",
    "\n",
    "Here we will review the accuracy of the model on both the training and test data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy:  0.9198281764984131\n",
      "Testing Accuracy:  0.8683457374572754\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the model on the training and testing set\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Training Accuracy: \", score[1])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Testing Accuracy: \", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Training and Testing accuracy scores are both high and an increase on our initial model. Training accuracy has increased by ~6% and Testing accuracy has increased by ~4%. \n",
    "\n",
    "There is a marginal increase in the difference between the Training and Test scores (~6% compared to ~5% previously) though the difference remains low so the model has not suffered from overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions  \n",
    "\n",
    "Here we will modify our previous method for testing the models predictions on a specified audio .wav file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_prediction(file_name):\n",
    "    prediction_feature = extract_features(file_name) \n",
    "    prediction_feature = prediction_feature.reshape(1, num_rows, num_columns, num_channels)\n",
    "\n",
    "    predicted_vector = model.predict_classes(prediction_feature)\n",
    "    predicted_class = le.inverse_transform(predicted_vector) \n",
    "    print(\"The predicted class is:\", predicted_class[0], '\\n') \n",
    "\n",
    "    predicted_proba_vector = model.predict_proba(prediction_feature) \n",
    "    predicted_proba = predicted_proba_vector[0]\n",
    "    for i in range(len(predicted_proba)): \n",
    "        category = le.inverse_transform(np.array([i]))\n",
    "        print(category[0], \"\\t\\t : \", format(predicted_proba[i], '.32f') )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation \n",
    "\n",
    "#### Test with sample data \n",
    "\n",
    "As before we will verify the predictions using a subsection of the sample audio files we explored in the first notebook. We expect the bulk of these to be classified correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-16-26df164fdc76>:5: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "The predicted class is: air_conditioner \n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-16-26df164fdc76>:9: Sequential.predict_proba (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use `model.predict()` instead.\n",
      "air_conditioner \t\t :  0.91008162498474121093750000000000\n",
      "car_horn \t\t :  0.00050033943261951208114624023438\n",
      "children_playing \t\t :  0.03129843249917030334472656250000\n",
      "dog_bark \t\t :  0.00170055136550217866897583007812\n",
      "drilling \t\t :  0.02245077863335609436035156250000\n",
      "engine_idling \t\t :  0.00291876145638525485992431640625\n",
      "gun_shot \t\t :  0.00025273195933550596237182617188\n",
      "jackhammer \t\t :  0.02114298008382320404052734375000\n",
      "siren \t\t :  0.00871215853840112686157226562500\n",
      "street_music \t\t :  0.00094158545834943652153015136719\n"
     ]
    }
   ],
   "source": [
    "# Class: Air Conditioner\n",
    "\n",
    "filename = '../UrbanSound Dataset sample/audio/100852-0-0-0.wav' \n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: drilling \n",
      "\n",
      "air_conditioner \t\t :  0.00001428662653779610991477966309\n",
      "car_horn \t\t :  0.00001807990884117316454648971558\n",
      "children_playing \t\t :  0.00000809219181974185630679130554\n",
      "dog_bark \t\t :  0.00000011522907072958332719281316\n",
      "drilling \t\t :  0.99887949228286743164062500000000\n",
      "engine_idling \t\t :  0.00001036807134369155392050743103\n",
      "gun_shot \t\t :  0.00001112346490117488428950309753\n",
      "jackhammer \t\t :  0.00051710568368434906005859375000\n",
      "siren \t\t :  0.00000012721483244604314677417278\n",
      "street_music \t\t :  0.00054121948778629302978515625000\n"
     ]
    }
   ],
   "source": [
    "# Class: Drilling\n",
    "\n",
    "filename = '../UrbanSound Dataset sample/audio/103199-4-0-0.wav'\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: street_music \n",
      "\n",
      "air_conditioner \t\t :  0.00128569686785340309143066406250\n",
      "car_horn \t\t :  0.00452070496976375579833984375000\n",
      "children_playing \t\t :  0.09430360049009323120117187500000\n",
      "dog_bark \t\t :  0.03333890065550804138183593750000\n",
      "drilling \t\t :  0.00023773724387865513563156127930\n",
      "engine_idling \t\t :  0.00010984209075104445219039916992\n",
      "gun_shot \t\t :  0.00000000280732526114491065527545\n",
      "jackhammer \t\t :  0.00001063408763002371415495872498\n",
      "siren \t\t :  0.01030355598777532577514648437500\n",
      "street_music \t\t :  0.85588932037353515625000000000000\n"
     ]
    }
   ],
   "source": [
    "# Class: Street music \n",
    "\n",
    "filename = '../UrbanSound Dataset sample/audio/101848-9-0-0.wav'\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: jackhammer \n",
      "\n",
      "air_conditioner \t\t :  0.00465527782216668128967285156250\n",
      "car_horn \t\t :  0.12551957368850708007812500000000\n",
      "children_playing \t\t :  0.01239559240639209747314453125000\n",
      "dog_bark \t\t :  0.14426797628402709960937500000000\n",
      "drilling \t\t :  0.20020681619644165039062500000000\n",
      "engine_idling \t\t :  0.02069007605314254760742187500000\n",
      "gun_shot \t\t :  0.12677641212940216064453125000000\n",
      "jackhammer \t\t :  0.32507652044296264648437500000000\n",
      "siren \t\t :  0.03476354479789733886718750000000\n",
      "street_music \t\t :  0.00564818549901247024536132812500\n"
     ]
    }
   ],
   "source": [
    "# Class: Car Horn \n",
    "\n",
    "filename = '../UrbanSound Dataset sample/audio/100648-1-0-0.wav'\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations \n",
    "\n",
    "We can see that the model performs well. \n",
    "\n",
    "Interestingly, car horn was again incorrectly classifed but this time as drilling - though the per class confidence shows it was a close decision between car horn with 26% confidence and drilling at 34% confidence.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other audio\n",
    "\n",
    "Again we will further validate our model using a sample of various copyright free sounds that we not part of either our test or training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: dog_bark \n",
      "\n",
      "air_conditioner \t\t :  0.00143127748742699623107910156250\n",
      "car_horn \t\t :  0.04909551888704299926757812500000\n",
      "children_playing \t\t :  0.00542677706107497215270996093750\n",
      "dog_bark \t\t :  0.80641847848892211914062500000000\n",
      "drilling \t\t :  0.10159351676702499389648437500000\n",
      "engine_idling \t\t :  0.00020524892897810786962509155273\n",
      "gun_shot \t\t :  0.02920273877680301666259765625000\n",
      "jackhammer \t\t :  0.00046065766946412622928619384766\n",
      "siren \t\t :  0.00367915839888155460357666015625\n",
      "street_music \t\t :  0.00248655420728027820587158203125\n"
     ]
    }
   ],
   "source": [
    "filename = '../Evaluation audio/dog_bark_1.wav'\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: jackhammer \n",
      "\n",
      "air_conditioner \t\t :  0.03098107129335403442382812500000\n",
      "car_horn \t\t :  0.00001872309258033055812120437622\n",
      "children_playing \t\t :  0.00028658681549131870269775390625\n",
      "dog_bark \t\t :  0.00064256438054144382476806640625\n",
      "drilling \t\t :  0.00351485470309853553771972656250\n",
      "engine_idling \t\t :  0.00356060685589909553527832031250\n",
      "gun_shot \t\t :  0.00000060925168554604169912636280\n",
      "jackhammer \t\t :  0.96077275276184082031250000000000\n",
      "siren \t\t :  0.00006042141831130720674991607666\n",
      "street_music \t\t :  0.00016177661018446087837219238281\n"
     ]
    }
   ],
   "source": [
    "filename = '../Evaluation audio/drilling_1.wav'\n",
    "\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted class is: gun_shot \n",
      "\n",
      "air_conditioner \t\t :  0.00060294655850157141685485839844\n",
      "car_horn \t\t :  0.00108347751665860414505004882812\n",
      "children_playing \t\t :  0.00439422763884067535400390625000\n",
      "dog_bark \t\t :  0.06342021375894546508789062500000\n",
      "drilling \t\t :  0.18518704175949096679687500000000\n",
      "engine_idling \t\t :  0.01274545863270759582519531250000\n",
      "gun_shot \t\t :  0.72738718986511230468750000000000\n",
      "jackhammer \t\t :  0.00005645949568133801221847534180\n",
      "siren \t\t :  0.00274454662576317787170410156250\n",
      "street_music \t\t :  0.00237840879708528518676757812500\n"
     ]
    }
   ],
   "source": [
    "filename = '../Evaluation audio/gun_shot_1.wav'\n",
    "\n",
    "print_prediction(filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations \n",
    "\n",
    "The performance of our final model is very good and has generalised well, seeming to predict well when tested against new audio data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
